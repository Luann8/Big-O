
<h1>Algorithm Complexity Examples</h1>

<h2>Algorithm 1: O(1) - Constant Time</h2>
<h3>Description:</h3>
<p>The execution time remains constant, regardless of the size of the input. A typical example is accessing a specific element in an array by index.</p>

<h2>Algorithm 2: O(log n) - Logarithmic Time</h2>
<h3>Description:</h3>
<p>The execution time grows logarithmically with the size of the input. An example is binary search in a sorted list.</p>

<h2>Algorithm 3: O(n) - Linear Time</h2>
<h3>Description:</h3>
<p>The execution time grows linearly with the size of the input. An example is iterating through an array and performing an operation on each element.</p>

<h2>Algorithm 4: O(n log n) - Log-Linear Time</h2>
<h3>Description:</h3>
<p>The execution time grows faster than logarithmic but slower than polynomial. An example is the Merge Sort algorithm.</p>

<h2>Algorithm 5: O(nÂ²) - Quadratic Time</h2>
<h3>Description:</h3>
<p>The execution time grows quadratically with the size of the input. An example is the Insertion Sort algorithm.</p>

<h2>Algorithm 6: O(2^n) - Exponential Time</h2>
<h3>Description:</h3>
<p>The execution time grows exponentially with the size of the input. An example is solving the Fibonacci sequence using recursion.</p>

<h2>Algorithm 7: O(n!) - Factorial Time</h2>
<h3>Description:</h3>
<p>The time complexity grows extremely fast, making it impractical for large datasets. An example is solving the Traveling Salesman Problem using brute force.</p>

<h2>Discussion:</h2>
<p>In summary, different algorithms exhibit different time complexities, affecting their efficiency for various problem sizes. Constant and logarithmic time algorithms are highly efficient, while quadratic, exponential, and factorial time algorithms become increasingly impractical as the input size grows.</p>
